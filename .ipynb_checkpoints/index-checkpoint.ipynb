{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASHRAE ENERGY PREDICTION.\n",
    "\n",
    "This competition is being hosted on kaggle and I have decided to participate. We are given sets of data containing weather and site information\n",
    "\n",
    "Competition link: https://www.kaggle.com/c/ashrae-energy-prediction\n",
    "\n",
    "Data set link: https://www.kaggle.com/c/ashrae-energy-prediction/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import pickle\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "# import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data. The data I used was split in a different kernel using pandas df['timestamp'].dt.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= pd.read_pickle(\"../data/input/df_train.pkl\")\n",
    "df_test = pd.read_pickle(\"../data/input/df_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_2 = pd.read_pickle(\"../data/input/df_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_2.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function defined below is used to reduce the byte memory consumption.\n",
    "It works by converting the data type of the columns into the minimum permitted data type in order to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            # skip datetime type or categorical type\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had a lot of missing data in the dataset. I filled the missing data with the global average value per day which is 22.4 for air temperature, 16 for dew temperature and converted the year feature into the difference between the year and 1900. Note: 1900 is the year the oldest building in the data was built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_col(df, col, val):\n",
    "    df[col] = df[col].fillna(val)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = fill_col(df_train, \"air_temperature\", 22.4)\n",
    "df_test = fill_col(df_test, \"air_temperature\", 22.4)\n",
    "\n",
    "df_train['year_built'] = df_train['year_built'] - 1900\n",
    "df_test['year_built'] = df_test['year_built'] - 1900\n",
    "\n",
    "df_train = fill_col(df_train, 'dew_temperature', 16)\n",
    "df_test = fill_col(df_test, 'dew_temperature', 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for feature selection, after going through the competitions discussion, I noticed a correlation amongst the claims of high ranking competitors about which features were important to the model and which were not, I selected these features and used them for my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_feats_train = [\n",
    "    'building_id',\n",
    "    'square_feet',\n",
    "    'meter',\n",
    "    'meter_reading',\n",
    "    'air_temperature',\n",
    "    'dew_temperature',\n",
    "    'floor_count',\n",
    "    'primary_use',\n",
    "    'year_built',\n",
    "    'DT_hour',\n",
    "    'site_id',\n",
    "    'DT_day_week', # bottom3 FI\n",
    "#     'cloud_coverage', # bottom3 FI\n",
    "#     'precip_depth_1_hr' # bottom3 FI\n",
    "]\n",
    "\n",
    "good_feats_test = [\n",
    "    'row_id',\n",
    "    'building_id',\n",
    "    'square_feet',\n",
    "    'meter',\n",
    "    'air_temperature',\n",
    "    'dew_temperature',\n",
    "    'floor_count',\n",
    "    'primary_use',\n",
    "    'year_built',\n",
    "    'DT_hour',\n",
    "    'site_id',\n",
    "    'DT_day_week', # bottom3 FI\n",
    "#     'cloud_coverage', # bottom3 FI\n",
    "#     'precip_depth_1_hr' # bottom3 FI\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_feats(df, feats):\n",
    "    df = df[feats].copy()\n",
    "    return df\n",
    "\n",
    "# df_train_target = df_train['meter_reading'].copy()\n",
    "df_train = select_feats(df_train, good_feats_train)\n",
    "df_test = select_feats(df_test, good_feats_test)\n",
    "# df_train['meter_reading'] = df_train_target\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally in machine learning competitions, it is advisable to convert your predictions target to reflect the evaluation being used in prediction loss. The competition stated that it uses the Root Mean Squared Logarithmic Error for evaluation so that is what was used to transform the meter readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['meter_reading_log1p'] = np.log1p(df_train['meter_reading'])\n",
    "df_train = df_train.drop(\"meter_reading\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "This is where the action starts.\n",
    "As Andrew Ng said, \"Applied machine learning is basically feature engineering\"\n",
    "After training the whole dataset using lgbm and checking the feature importance, the most important feature was the building id. This seems quite logical because energy consumption is based on building materials, number of people that contribute to the usage and so on.\n",
    "I started by generating features based on building id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Merge Columns to generate new features\n",
    "\"\"\"\n",
    "def generate_merge_feats(df):\n",
    "    df['building_meter'] = (df['building_id'].map(str) + '_' + df['meter'].map(str)).astype('category')\n",
    "    df['building_day_week'] = (df['building_id'].map(str) + '_' + df['DT_day_week'].map(str)).astype('category')\n",
    "    return df\n",
    "\n",
    "df_train = generate_merge_feats(df_train)\n",
    "df_test = generate_merge_feats(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate statistical based features\n",
    "\"\"\"\n",
    "gc.collect()\n",
    "\n",
    "building_meter_group = df_train.groupby('building_meter')['meter_reading_log1p']\n",
    "building_mean_per_meter = building_meter_group.transform('mean').astype(np.float16)\n",
    "building_median_per_meter = building_meter_group.transform('median').astype(np.float16)\n",
    "building_min_per_meter = building_meter_group.transform('min').astype(np.float16)\n",
    "building_max_per_meter = building_meter_group.transform('max').astype(np.float16)\n",
    "building_count_per_meter = building_meter_group.transform('count').astype(np.float16)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Map these features to their conterparts\n",
    "\"\"\"\n",
    "def map_stats_feats(df):\n",
    "    df['building_mean_per_meter'] = df['building_id'].map(building_mean_per_meter)\n",
    "    df['building_median_per_meter'] = df['building_id'].map(building_median_per_meter)\n",
    "    df['building_min_per_meter'] = df['building_id'].map(building_min_per_meter)\n",
    "    df['building_max_per_meter'] = df['building_id'].map(building_max_per_meter)\n",
    "    df['building_count_per_meter'] = df['building_id'].map(building_count_per_meter)\n",
    "    return df\n",
    "    \n",
    "df_train = map_stats_feats(df_train)\n",
    "df_test = map_stats_feats(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Also create weather based features based on site id\n",
    "\"\"\"\n",
    "def create_weather_feats(data):\n",
    "    data['air_temperature_mean'] = data.groupby('site_id')['air_temperature'].transform(\"mean\").astype(np.float16)\n",
    "    data['air_temperature_min'] = data.groupby('site_id')['air_temperature'].transform(\"min\").astype(np.float16)\n",
    "    data['air_temperature_max'] = data.groupby('site_id')['air_temperature'].transform(\"max\").astype(np.float16)\n",
    "    data['dew_temperature_mean'] = data.groupby('site_id')['dew_temperature'].transform(\"mean\").astype(np.float16)\n",
    "    data['dew_temperature_min'] = data.groupby('site_id')['dew_temperature'].transform(\"min\").astype(np.float16)\n",
    "    data['dew_temperature_max'] = data.groupby('site_id')['dew_temperature'].transform(\"max\").astype(np.float16)\n",
    "    return data\n",
    "    \n",
    "df_train = create_weather_feats(df_train)\n",
    "df_test = create_weather_feats(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add rolling features of air temperature and dew temperature at varying windows\n",
    "\"\"\"\n",
    "def add_lag_features(df, window=3):\n",
    "    group_df = df.groupby(['building_day_week'])\n",
    "    cols = ['air_temperature', 'dew_temperature']\n",
    "    rolled = group_df[cols].rolling(window=window, min_periods=0)\n",
    "    lag_mean = rolled.mean().reset_index().astype(np.float16)\n",
    "    lag_max = rolled.max().reset_index().astype(np.float16)\n",
    "    lag_min = rolled.min().reset_index().astype(np.float16)\n",
    "    lag_std = rolled.std().reset_index().astype(np.float16)\n",
    "    for col in cols:\n",
    "        df[f'{col}_mean_lag{window}'] = lag_mean[col]\n",
    "        df[f'{col}_max_lag{window}'] = lag_max[col]\n",
    "        df[f'{col}_min_lag{window}'] = lag_min[col]\n",
    "        df[f'{col}_std_lag{window}'] = lag_std[col]\n",
    "    return df\n",
    "        \n",
    "\n",
    "df_train = add_lag_features(df_train, window=72)\n",
    "df_test = add_lag_features(df_test, window=72)\n",
    "# df_train = add_lag_features(df_train, window=168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['meter_reading_log1p_std_lag72'], axis='columns')\n",
    "df_test = df_test.drop(['meter_reading_log1p_std_lag72'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['dew_temperature_std_lag72'], axis='columns')\n",
    "df_test = df_test.drop(['dew_temperature_std_lag72'], axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('reducing mem usage for df_train...')\n",
    "df_train = reduce_mem_usage(df_train, use_float16=True)\n",
    "\n",
    "print('reducing mem usage for df_test...')\n",
    "df_test = reduce_mem_usage(df_test, use_float16=True)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training of the lgb model will be done based on meter type. This is referred to as a batch. Each training batch will be trained using a time series split of 3 since this is a time series competition and each training split will produce a model which will be saved into a list. Predictions will then be made using these three models and averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_cols = [\"building_id\", \"site_id\", \"primary_use\", \"building_meter\", \"building_day_week\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_train_data_batch(data, target_meter):\n",
    "    to_train = data.loc[data['meter'] == target_meter]\n",
    "    to_train_target = to_train['meter_reading_log1p'].values\n",
    "    to_train_data = to_train.drop('meter_reading_log1p', axis='columns')\n",
    "    del to_train\n",
    "    gc.collect()\n",
    "    return to_train_data, to_train_target\n",
    "\n",
    "def fetch_test_data_batch(test_data, target_meter):\n",
    "    to_test_data = test_data.loc[test_data['meter'] == target_meter]\n",
    "    return to_test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lgbm(train, val, devices=(-1), seed=None, cat_features=None, num_rounds=1500, lr=0.001, bf=0.1):\n",
    "    \"\"\"Function to train the Light GBM model\"\"\"\n",
    "    X_tt, y_tt = train\n",
    "    X_vl, y_vl = val\n",
    "    metric = 'l2'\n",
    "#     params = {\n",
    "#         'objective':'regression',\n",
    "#         'boosting_type':'gbdt',\n",
    "#         'learning_rate':lr,\n",
    "#         'num_leaves': 2**8,\n",
    "#         'max_depth':20,\n",
    "#         'n_estimators':5000,\n",
    "#         'max_bin':255,\n",
    "#         'num_leaves': 20,\n",
    "#         'reg_alpha': 0.1,\n",
    "#         'reg_lambda': 0.3,\n",
    "#         'verbose':-1,\n",
    "#         'seed': 42,\n",
    "#         \"bagging_freq\": 5,\n",
    "#         \"bagging_fraction\": bf,\n",
    "#         \"feature_fraction\": 0.9,\n",
    "#         \"metric\": metric,\n",
    "#         'early_stopping_rounds':100\n",
    "#     }\n",
    "    params = {\n",
    "        'objective':'regression',\n",
    "        'boosting_type':'gbdt',\n",
    "        'metric': metric,\n",
    "        'learning_rate':0.07,\n",
    "        'num_leaves': 2**8,\n",
    "        'max_depth':20, #-1\n",
    "        'colsample_bytree': 0.9,\n",
    "        'subsample_freq':1,\n",
    "        'subsample':0.5,\n",
    "        'n_estimators':5000,\n",
    "        'max_bin':255,\n",
    "        'num_leaves': 20,\n",
    "        'verbose':-1,\n",
    "        'seed': 42,\n",
    "        'early_stopping_rounds':100,\n",
    "    }\n",
    "    device = devices\n",
    "    if device == -1:\n",
    "        pass # use cpu\n",
    "    else:\n",
    "        print(f\"using gpu {device}...\") # use gpu\n",
    "        params.update({'device': 'gpu', 'gpu_device_id': device})\n",
    "    params[\"seed\"] = seed\n",
    "    d_train = lgb.Dataset(X_tt, label=y_tt, categorical_feature=cat_features)\n",
    "    d_valid = lgb.Dataset(X_vl, label=y_vl, categorical_feature=cat_features)\n",
    "    watchlist = [d_train, d_valid]\n",
    "    print(\"training LGB: \")\n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      valid_sets=watchlist,\n",
    "                      verbose_eval=100)\n",
    "    print(\"best score\", model.best_score)\n",
    "    log = {'train/mae': model.best_score['training'][metric],\n",
    "           'valid/mae': model.best_score['valid_1'][metric]}\n",
    "    return model, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3 things to change: target_meter, models0 and models0.append\n",
    "\"\"\"\n",
    "target_meter = 0\n",
    "models0 = []\n",
    "\n",
    "X_train, y_train = fetch_train_data_batch(df_train, target_meter)\n",
    "y_valid_pred_total = np.zeros(X_train.shape[0])\n",
    "gc.collect()\n",
    "print(\"The target meter is : \", target_meter)\n",
    "\n",
    "cat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "for train_idx, val_idx in tscv.split(X_train):\n",
    "    train_data = X_train.iloc[train_idx], y_train[train_idx]\n",
    "    val_data = X_train.iloc[val_idx], y_train[val_idx]\n",
    "    print(\"Training: \", len(train_idx), \" and validating: \", len(val_idx))\n",
    "    model, log = fit_lgbm(train_data,val_data,cat_features=category_cols,num_rounds=1000,lr=0.05,bf=0.7)\n",
    "    models0.append(model)\n",
    "    gc.collect()\n",
    "print(\"training has ended\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3 things to change: target_meter, models0 and models0.append\n",
    "\"\"\"\n",
    "target_meter = 1\n",
    "models1 = []\n",
    "\n",
    "X_train, y_train = fetch_train_data_batch(df_train, target_meter)\n",
    "y_valid_pred_total = np.zeros(X_train.shape[0])\n",
    "gc.collect()\n",
    "print(\"The target meter is : \", target_meter)\n",
    "\n",
    "cat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "for train_idx, val_idx in tscv.split(X_train):\n",
    "    train_data = X_train.iloc[train_idx], y_train[train_idx]\n",
    "    val_data = X_train.iloc[val_idx], y_train[val_idx]\n",
    "    print(\"Training: \", len(train_idx), \" and validating: \", len(val_idx))\n",
    "    model, log = fit_lgbm(train_data,val_data,cat_features=category_cols,num_rounds=1000,lr=0.05,bf=0.7)\n",
    "    models1.append(model)\n",
    "    gc.collect()\n",
    "print(\"training has ended\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3 things to change: target_meter, models0 and models0.append\n",
    "\"\"\"\n",
    "target_meter = 2\n",
    "models2 = []\n",
    "\n",
    "X_train, y_train = fetch_train_data_batch(df_train, target_meter)\n",
    "gc.collect()\n",
    "print(\"The target meter is : \", target_meter)\n",
    "\n",
    "cat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "for train_idx, val_idx in tscv.split(X_train):\n",
    "    train_data = X_train.iloc[train_idx], y_train[train_idx]\n",
    "    val_data = X_train.iloc[val_idx], y_train[val_idx]\n",
    "    print(\"Training: \", len(train_idx), \" and validating: \", len(val_idx))\n",
    "    model, log = fit_lgbm(train_data,val_data,cat_features=category_cols,num_rounds=1000,lr=0.05,bf=0.7)\n",
    "    models2.append(model)\n",
    "    gc.collect()\n",
    "print(\"training for meter type has ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3 things to change: target_meter, models0 and models0.append\n",
    "\"\"\"\n",
    "target_meter = 3\n",
    "models3 = []\n",
    "\n",
    "X_train, y_train = fetch_train_data_batch(df_train, target_meter)\n",
    "y_valid_pred_total = np.zeros(X_train.shape[0])\n",
    "gc.collect()\n",
    "print(\"The target meter is : \", target_meter)\n",
    "\n",
    "cat_features = [X_train.columns.get_loc(cat_col) for cat_col in category_cols]\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "for train_idx, val_idx in tscv.split(X_train):\n",
    "    train_data = X_train.iloc[train_idx], y_train[train_idx]\n",
    "    val_data = X_train.iloc[val_idx], y_train[val_idx]\n",
    "    print(\"Training: \", len(train_idx), \" and validating: \", len(val_idx))\n",
    "    model, log = fit_lgbm(train_data,val_data,cat_features=category_cols,num_rounds=1000,lr=0.05,bf=0.7)\n",
    "    models3.append(model)\n",
    "    gc.collect()\n",
    "print(\"training has ended\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_models_v1 = [models0, models1, models2, models3]\n",
    "pickle.dump(lgb_models_v1, open('../output/lgb_models_v1.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions were made using the three models and the average was taken. It was then converted back to it's antilogarithmic form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(X_pred, models):\n",
    "    X_pred_id = X_pred['row_id'] # returning the row_id in order to preserve order\n",
    "    X_pred = X_pred.drop('row_id', axis='columns')\n",
    "    preds_arr = []\n",
    "    final_predictions_total = np.zeros(X_pred.shape[0])\n",
    "    for i in range(len(models)):\n",
    "        print(f\"Making predictions for model {i}\")\n",
    "        estimator = models[i]\n",
    "        predictions = estimator.predict(X_pred, num_iteration=estimator.best_iteration)\n",
    "        final_predictions_total += predictions\n",
    "    # Now we are done with the predictions, we'll take the average of the predictions\n",
    "    final_predictions_total /= len(models)\n",
    "    final_predictions_total = np.expm1(final_predictions_total)\n",
    "    return X_pred_id, final_predictions_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_row_ids = []\n",
    "all_predictions = []\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To start predicting for the test set, 2 things to change\n",
    "\"\"\"\n",
    "target_meter = 0\n",
    "use_model = models0\n",
    "\n",
    "X_test = fetch_test_data_batch(df_test, target_meter)\n",
    "y_test_id, y_test = make_predictions(X_test, use_model)\n",
    "\n",
    "all_row_ids.append(y_test_id)\n",
    "all_predictions.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To start predicting for the test set, 2 things to change\n",
    "\"\"\"\n",
    "target_meter = 1\n",
    "use_model = models1\n",
    "\n",
    "X_test = fetch_test_data_batch(df_test, target_meter)\n",
    "y_test_id, y_test = make_predictions(X_test, use_model)\n",
    "\n",
    "all_row_ids.append(y_test_id)\n",
    "all_predictions.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To start predicting for the test set, 2 things to change\n",
    "\"\"\"\n",
    "target_meter = 2\n",
    "use_model = models2\n",
    "\n",
    "X_test = fetch_test_data_batch(df_test, target_meter)\n",
    "y_test_id, y_test = make_predictions(X_test, use_model)\n",
    "\n",
    "all_row_ids.append(y_test_id)\n",
    "all_predictions.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To start predicting for the test set, 2 things to change\n",
    "\"\"\"\n",
    "target_meter = 3\n",
    "use_model = models3\n",
    "\n",
    "X_test = fetch_test_data_batch(df_test, target_meter)\n",
    "y_test_id, y_test = make_predictions(X_test, use_model)\n",
    "\n",
    "all_row_ids.append(y_test_id)\n",
    "all_predictions.append(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_row_ids_flat = [ids for sublist in all_row_ids for ids in sublist]\n",
    "all_predictions_flat = [preds for predsublist in all_predictions for preds in predsublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the submission in the appropriate format and submit to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({})\n",
    "submission['row_id'] = all_row_ids_flat\n",
    "submission['meter_reading'] = all_predictions_flat\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"../output/lgb_models_v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions submit ashrae-energy-prediction -f ../output/lgb_models_v2.csv -m \"lgb_models_v2\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
